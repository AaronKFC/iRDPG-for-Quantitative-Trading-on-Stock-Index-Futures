@inproceedings{liu2020adaptive,
  title={Adaptive quantitative trading: an imitative deep reinforcement learning approach},
  author={Liu, Yang and Liu, Qi and Zhao, Hongke and Pan, Zhen and Liu, Chuanren},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={02},
  pages={2128--2135},
  year={2020}
}

@article{moody1998performance,
  title={Performance functions and reinforcement learning for trading systems and portfolios},
  author={Moody, John and Wu, Lizhong and Liao, Yuansong and Saffell, Matthew},
  journal={Journal of Forecasting},
  volume={17},
  number={5-6},
  pages={441--470},
  year={1998},
  publisher={Wiley Online Library}
}

@article{heess2015memory,
  title={Memory-based control with recurrent neural networks},
  author={Heess, Nicolas and Hunt, Jonathan J and Lillicrap, Timothy P and Silver, David},
  journal={arXiv preprint arXiv:1512.04455},
  year={2015}
}

@article{schaul2015prioritized,
  title={Prioritized experience replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  journal={arXiv preprint arXiv:1511.05952},
  year={2015}
}

@article{vecerik2017leveraging,
  title={Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards},
  author={Vecerik, Mel and Hester, Todd and Scholz, Jonathan and Wang, Fumin and Pietquin, Olivier and Piot, Bilal and Heess, Nicolas and Roth{\"o}rl, Thomas and Lampe, Thomas and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1707.08817},
  year={2017}
}

@inproceedings{ross2010efficient,
  title={Efficient reductions for imitation learning},
  author={Ross, St{\'e}phane and Bagnell, Drew},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={661--668},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}

@inproceedings{kang2018policy,
  title={Policy optimization with demonstrations},
  author={Kang, Bingyi and Jie, Zequn and Feng, Jiashi},
  booktitle={International Conference on Machine Learning},
  pages={2469--2478},
  year={2018},
  organization={PMLR}
}

@article{KAELBLING199899,
title = {Planning and acting in partially observable stochastic domains},
journal = {Artificial Intelligence},
volume = {101},
number = {1},
pages = {99-134},
year = {1998},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(98)00023-X},
url = {https://www.sciencedirect.com/science/article/pii/S000437029800023X},
author = {Leslie Pack Kaelbling and Michael L. Littman and Anthony R. Cassandra},
keywords = {Planning, Uncertainty, Partially observable Markov decision processes},
abstract = {In this paper, we bring techniques from operations research to bear on the problem of choosing optimal actions in partially observable stochastic domains. We begin by introducing the theory of Markov decision processes (mdps) and partially observable MDPs (pomdps). We then outline a novel algorithm for solving pomdps off line and show how, in some cases, a finite-memory controller can be extracted from the solution to a POMDP. We conclude with a discussion of how our approach relates to previous work, the complexity of finding exact solutions to pomdps, and of some possibilities for finding approximate solutions.}
}%  

@article{DBLP:journals/corr/HeessHLS15,
  author    = {Nicolas Heess and
               Jonathan J. Hunt and
               Timothy P. Lillicrap and
               David Silver},
  title     = {Memory-based control with recurrent neural networks},
  journal   = {CoRR},
  volume    = {abs/1512.04455},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.04455},
  archivePrefix = {arXiv},
  eprint    = {1512.04455},
  timestamp = {Mon, 13 Aug 2018 16:47:14 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HeessHLS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{joinQuant,
  title={Dataset link},
  author={JoinQuant},
  url={https://www.joinquant.com/help/api/help#name:Future},
}